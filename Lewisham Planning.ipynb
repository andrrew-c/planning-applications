{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os, sys\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "# My functions\n",
    "import lewisham_functions as lf\n",
    "\n",
    "import functions as mf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Options\n",
    "urlbase = \"https://planning.lewisham.gov.uk/online-applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postcode to search\n",
    "postcode = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium option\n",
    "Load up planning web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Init browser object on planning page\n",
    "browser = initbrowser(urlbase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter in postcode details and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With browser object make search\n",
    "lf.makeSearch(postcode, browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SE263 only returned a single result.\n",
    "# Check if only one application comes up\n",
    "#def onSearchPage(browser):\n",
    "\n",
    "\n",
    "# tag = browser.find_elements_by_xpath(\"//*[contains(text(), 'Application Summary')]\")\n",
    "# tag[0].text\n",
    "\n",
    "#browser.find_elements_by_xpath(\"//*[contains(text(), 'Planning')]\")\n",
    "searchResults = lf.hasMultipleResults(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update search results to 100 on a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if searchResults:\n",
    "    lf.makeResults100(browser)\n",
    "#     numresults = browser.find_element_by_xpath(\"//select[@id='resultsPerPage']\")\n",
    "#     numresults.send_keys('100')\n",
    "#     browser.find_element_by_xpath(\"//input[@type='submit']\").click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's get the number of pages of results for this postcode and then the number of results in total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultPages = lf.getResultNumber(browser, searchResults)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's find the applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract applications via selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Iterate through all search pages and get links\n",
    "hrefs = lf.getSearchResults(browser, searchResults, resultPages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save pickled object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lf.saveLinks(hrefs, postcode) #os.path.abspath(os.curdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get application information via pickled object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(lf)\n",
    "hrefs = lf.loadLinks(postcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get information on each application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = \"Summary,Further Information,Contacts,Important Dates\".split(',')\n",
    "\n",
    "# Load up application\n",
    "#app = hrefs[0]\n",
    "#app\n",
    "#browser.get(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTableFromPage(transp=True):\n",
    "    \n",
    "    \"\"\" Assumes a live browser object (selenium)\n",
    "        returns a table extracted from the HTML\n",
    "    \"\"\"\n",
    "    \n",
    "    # Init table\n",
    "    table = None\n",
    "    \n",
    "    # Let's load a table into a pd dataframe\n",
    "    html = browser.page_source\n",
    "    soup = bs(html, 'html.parser')\n",
    "    div = soup.find('table')\n",
    "    if div !=None:\n",
    "        table = pd.read_html(str(div))[0]\n",
    "        #table.iloc[:,0]\n",
    "        #table.columns = table.iloc[:,0]\n",
    "        #table.drop(index=0, inplace=True)\n",
    "        if transp:\n",
    "            table = table.transpose() \n",
    "        table.columns =table.iloc[0,:]\n",
    "        table.drop(index=0, inplace=True)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetailsMultiplePages(links):\n",
    "    \n",
    "    \"\"\"\n",
    "        Loops over multiple pages to get information from the links\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initialise dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for link in tqdm(links):\n",
    "\n",
    "        # Load up application\n",
    "        app = link\n",
    "        \n",
    "        try:\n",
    "            browser.get(app)\n",
    "\n",
    "            ## Initiliase table\n",
    "            t1 = GetTableFromPage()\n",
    "\n",
    "            ## Save URL\n",
    "            t1.url = app\n",
    "\n",
    "            ## Iterate over remaining tab names\n",
    "            for t in tabs[1:]:\n",
    "\n",
    "                #print(t)\n",
    "                xp = \"//span[contains(text(), '{}')]\".format(t)\n",
    "\n",
    "                btn = browser.find_element_by_xpath(xp)\n",
    "                tabLink = btn.find_element_by_xpath('./..').get_attribute('href')\n",
    "                browser.get(tabLink)\n",
    "                newTable = GetTableFromPage()\n",
    "\n",
    "                if str(type(newTable)) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "                    ## Update column names if already in table\n",
    "                    newColumns = ['{}_{}'.format(t.replace(' ', '_'), c) if c in t1.columns else c for c in newTable.columns]\n",
    "                    newTable.columns = newColumns\n",
    "\n",
    "                    t1= t1.merge(newTable, 'outer', left_index=True, right_index=True)\n",
    "\n",
    "            # Remove all spaces from col names\n",
    "            newColumns = [col.replace(' ', '_') for col in t1.columns]\n",
    "            t1.columns = newColumns\n",
    "\n",
    "            ## If df hasn't been updated yet\n",
    "            if df.shape[0] == 0:\n",
    "                df = t1.copy()\n",
    "            else:\n",
    "\n",
    "                ## Add on row\n",
    "                df = df.append(t1, sort=False)\n",
    "\n",
    "            time.sleep(1)\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "        \n",
    "    return df\n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "# Let's run loop over each applications\n",
    "df =getDetailsMultiplePages(hrefs)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's save this object to pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveApplicationInfo(df):\n",
    "    \n",
    "    \"\"\" Save the application information that has been scraped by getDetailsMultiplePages \"\"\"\n",
    "    \n",
    "    fname = \"data_{}_{}.p\".format(postcode, datetime.today().strftime('%Y%m%d'))\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        print(\"File '{}' already exists'\")\n",
    "    else:\n",
    "\n",
    "        with open(fname, 'wb') as f: pickle.dump(df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveApplicationInfo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPDFs(link):\n",
    "    \n",
    "    ####################################\n",
    "    # Testing out get the documents etc.\n",
    "    ####################################\n",
    "    \n",
    "    ## Get page for application (loads up 'Summary tab')\n",
    "    browser.get(link)\n",
    "    \n",
    "    # Get case number\n",
    "    caseTag = browser.find_element_by_xpath(\"//span[@class='caseNumber']\").text\n",
    "    \n",
    "    \n",
    "    # Table\n",
    "    tabs = browser.find_element_by_xpath(\"//ul[@class='tabs']\")\n",
    "    \n",
    "    lists_all = tabs.find_elements_by_xpath(\".//li\")\n",
    "\n",
    "    ## Get direct children of section (where parents match)\n",
    "    lists = [l for l in lists_all if l.find_element_by_xpath(\"./..\") == tabs]\n",
    "\n",
    "    #[l.text for l in lists]\n",
    "\n",
    "    ##############################\n",
    "    # Load documents page\n",
    "    ##############################\n",
    "    \n",
    "    # How many documents?\n",
    "    numDocs = int(re.findall('[0-9]+', lists[3].text)[0])\n",
    "    if numDocs ==0:\n",
    "        print(\"Jobby\")\n",
    "        return None\n",
    "    else:\n",
    "        lists[3].click()\n",
    "\n",
    "        # Table with documents - no need to transpose before taking column names\n",
    "        docTable = GetTableFromPage(transp=False)\n",
    "\n",
    "        ## With PDF table assuming they are PDFs get the links\n",
    "        htmlTable = browser.find_element_by_xpath(\"//table[@id='Documents']\")\n",
    "\n",
    "        ## Assuming all docs are pdfs\n",
    "        links = htmlTable.find_elements_by_xpath(\".//a[contains(@href, 'pdf')]\")\n",
    "        #len(links)\n",
    "\n",
    "        ## Click each of the files to download\n",
    "        buttons = browser.find_elements_by_xpath(\"//input[contains(@onclick, 'buttonSwitch')]\")\n",
    "        for b in buttons:\n",
    "\n",
    "            b.click()\n",
    "\n",
    "        # Download as .zip\n",
    "        button = browser.find_element_by_xpath(\"//button[@type='submit'][@id='downloadFiles']\")\n",
    "        button.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        ## Get latest file \n",
    "        import glob \n",
    "        import shutil\n",
    "\n",
    "        dloadfiles = glob.glob(os.path.join(r'C:\\\\', 'users', 'andre', 'Downloads', '*.zip'))\n",
    "\n",
    "\n",
    "        latest_file = max(dloadfiles, key=os.path.getctime)\n",
    "        fname = latest_file.split('\\\\')[-1]\n",
    "        ## What is the full path?\n",
    "        os.path.abspath(latest_file)\n",
    "        ## Now move the file to current directory\n",
    "        shutil.move(latest_file, os.path.join(os.curdir, '.downloads', '{}_{}'.format(postcode,fname)))\n",
    "\n",
    "\n",
    "        # You've saved the PDFs, now keep the table\n",
    "        return docTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "for l in tqdm(hrefs[6:]):\n",
    "    getPDFs(l)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Iterate over tab names\n",
    "for t in tabs:\n",
    "\n",
    "    print(t)\n",
    "    xp = \"//span[contains(text(), '{}')]\".format(t)\n",
    "    #print(xp)\n",
    "    btn = browser.find_element_by_xpath(xp)\n",
    "    tabLink = btn.find_element_by_xpath('./..').get_attribute('href')\n",
    "    browser.get(tabLink)\n",
    "    newTable = GetTableFromPage()\n",
    "    \n",
    "    ## Update column names if already in table\n",
    "    newColumns = ['{}_{}'.format(t.replace(' ', '_'), c) if c in t1.columns else c for c in newTable.columns]\n",
    "    newTable.columns = newColumns\n",
    "    #print(t1.columns)\n",
    "    t1 = t1.merge(newTable, 'outer', left_index=True, right_index=True)\n",
    "    print(t1.shape)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
